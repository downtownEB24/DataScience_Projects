{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "32c357f5",
   "metadata": {},
   "source": [
    "# Machine Learning Project for Python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baf99735",
   "metadata": {},
   "source": [
    "Problem: \n",
    "\n",
    "A website sent advertisements by emails to users who interested in their product. Your task is to find a good model to predict if an advertisement will be clicked with given datasets. \n",
    "\n",
    "user_features.csv - features describing our users\n",
    "product_feature.csv - features describing products shown in the advertisements. \n",
    "click_history.csv - contains which products users had previously seen and whether that user ordered products in this website before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ac5e2952",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split,GridSearchCV\n",
    "from sklearn import metrics\n",
    "from sklearn import linear_model, tree\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "996e1e33",
   "metadata": {},
   "source": [
    "Question 1: Data Understanding\n",
    "Explore the basic information of the datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "56143b24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Click history has 35990 rows & 3 columns\n",
      "\n",
      "This dataframe of the file data as a whole has 107970 elements present\n",
      "\n",
      "Click history is made up of these data types:\n",
      " user_id       int64\n",
      "product_id    int64\n",
      "clicked        bool\n",
      "dtype: object\n",
      "\n",
      "First 25 unique user_ids in click history are:\n",
      " [104863 108656 100120 104838 107304 106682 110052 100142 101559 110501\n",
      " 109185 106343 107547 101696 111749 101080 111088 103033 100467 108385\n",
      " 111168 106003 102163 107394 100901]\n",
      "\n",
      "First 25 unique product ids in click history are:\n",
      " [1350 1321 1110 1443 1397 1246 1897 1843 1159 1268 1281 1670 1671 1228\n",
      " 1445 1384 1245 1404 1108 1855 1349 1649 1058 1527 1143]\n",
      "\n",
      "Summary statistics of data values in click history dataset are:\n",
      "              user_id    product_id\n",
      "count   35990.000000  35990.000000\n",
      "mean   106017.080161   1500.232898\n",
      "std      3483.480090    288.101984\n",
      "min    100001.000000   1000.000000\n",
      "25%    102976.500000   1250.000000\n",
      "50%    106060.000000   1503.000000\n",
      "75%    109049.000000   1749.000000\n",
      "max    111999.000000   1999.000000\n",
      "The amount of users that clicked on the ads were:  12688\n",
      "The amount of users that did not click on the ads were:  23302\n",
      "\n",
      "Product features has  1000 rows & 5 columns\n",
      "\n",
      "This dataframe of the file data as a whole has 5000 elements present\n",
      "\n",
      "Product features is made up of these data types:\n",
      " product_id             int64\n",
      "category              object\n",
      "on_sale                 bool\n",
      "number_of_reviews      int64\n",
      "avg_review_score     float64\n",
      "dtype: object\n",
      "\n",
      "First 25 unique product ids in product features are:\n",
      " [1134 1846 1762 1254 1493 1447 1647 1783 1093 1028 1827 1428 1753 1002\n",
      " 1439 1710 1225 1929 1885 1030 1024 1530 1918 1372 1211]\n",
      "\n",
      "The unique product categories in product features are:\n",
      " ['tools' 'skincare' 'fragrance' 'hair' 'body' 'foot' 'nail' 'makeup'\n",
      " 'men_skincare' 'hand' 'men_fragrance']\n",
      "\n",
      "The amount of products that were on sale were:  650\n",
      "The amount of products that were not on sale were:  350\n",
      "\n",
      "Summary statistics of data values in product features dataset are:\n",
      "         product_id  number_of_reviews  avg_review_score\n",
      "count  1000.000000       1.000000e+03       1000.000000\n",
      "mean   1499.500000       1.157725e+05          2.660656\n",
      "std     288.819436       5.028997e+05          1.741875\n",
      "min    1000.000000       6.600000e+01         -1.000000\n",
      "25%    1249.750000       2.570000e+02          1.428969\n",
      "50%    1499.500000       4.710000e+02          2.769397\n",
      "75%    1749.250000       7.042500e+02          4.180860\n",
      "max    1999.000000       2.307390e+06          5.000000\n",
      "\n",
      "User features has 12000 rows & 4 columns\n",
      "This dataframe of the file data as a whole has 48000 elements present\n",
      "\n",
      "User features is made up of these data types:\n",
      " user_id                     int64\n",
      "number_of_clicks_before    object\n",
      "ordered_before               bool\n",
      "personal_interests         object\n",
      "dtype: object\n",
      "\n",
      "First 25 unique user_ids in User features are:\n",
      " [104939 101562 102343 106728 107179 111516 106390 107303 101785 104961\n",
      " 103354 108337 103201 102570 104407 105639 104055 107258 102171 110542\n",
      " 108316 106405 100696 103904 108264]\n",
      "\n",
      "User rating score of the number of clicks made previously by users of previous ads:\n",
      " ['2' '5' '0' '4' '1' '6+' nan '3']\n",
      "\n",
      "How many products were ordered before by users were:  8400\n",
      "How many products were not ordered before by users were:  3600\n",
      "\n",
      "First 25 listed personal interests of each user:\n",
      " [\"['body', 'makeup', 'nail', 'hand', 'foot', 'men_fragrance', 'fragrance', 'hair', 'tools']\"\n",
      " \"['men_skincare', 'men_fragrance', 'tools', 'skincare', 'nail', 'body', 'makeup']\"\n",
      " \"['tools', 'makeup', 'foot', 'nail']\" \"['hand', 'men_skincare']\"\n",
      " \"['makeup', 'body', 'skincare', 'foot', 'men_skincare', 'fragrance', 'hair']\"\n",
      " \"['men_skincare', 'foot', 'fragrance', 'nail', 'hand', 'makeup', 'men_fragrance', 'tools']\"\n",
      " \"['makeup', 'men_skincare', 'fragrance', 'hand', 'foot', 'body', 'men_fragrance', 'skincare', 'tools', 'nail']\"\n",
      " \"['men_skincare', 'skincare', 'hand', 'men_fragrance', 'makeup', 'fragrance', 'body']\"\n",
      " \"['hair', 'hand']\" \"['nail', 'body', 'fragrance']\"\n",
      " \"['body', 'foot', 'hair', 'makeup']\"\n",
      " \"['hand', 'men_fragrance', 'men_skincare', 'nail', 'skincare', 'fragrance', 'foot', 'body']\"\n",
      " \"['body', 'men_skincare', 'fragrance', 'tools']\"\n",
      " \"['hair', 'men_skincare', 'hand', 'skincare', 'makeup', 'foot', 'fragrance', 'body', 'men_fragrance']\"\n",
      " \"['hair', 'men_skincare']\" \"['skincare', 'makeup']\"\n",
      " \"['hair', 'body', 'tools', 'nail', 'makeup', 'foot', 'fragrance', 'hand', 'men_fragrance']\"\n",
      " \"['men_fragrance', 'hand']\" \"['nail']\"\n",
      " \"['tools', 'men_skincare', 'hair', 'makeup', 'foot', 'fragrance', 'nail', 'body', 'men_fragrance']\"\n",
      " \"['foot', 'skincare', 'tools', 'hair', 'fragrance', 'hand', 'nail', 'body']\"\n",
      " \"['nail', 'hand', 'hair', 'makeup', 'men_skincare', 'skincare', 'fragrance', 'foot']\"\n",
      " \"['men_fragrance', 'hair', 'hand', 'fragrance', 'foot', 'men_skincare', 'body', 'makeup']\"\n",
      " \"['makeup']\" '[]']\n",
      "\n",
      "The number of users that do not have personal interests listed are:  1096\n"
     ]
    }
   ],
   "source": [
    "#Question 1.)\n",
    "#establishing file path of where input file is located\n",
    "path=\"C:\\\\Users\\esbro\\Desktop\\DAAN862\\Final Exam\"\n",
    "os.chdir(path) #changing current working directory to the path listed above \n",
    "hClick=pd.read_csv(\"click_history.csv\") #reading in input file from file path listed above\n",
    "hshape=hClick.shape #structure of input file (how many rows & columns)\n",
    "print(\"Click history has %5d rows & %1d columns\\n\" %(hshape[0],hshape[1]))\n",
    "hsize=hClick.size #column * rows= numerical number\n",
    "print(\"This dataframe of the file data as a whole has %6d elements present\\n\" % hsize)\n",
    "#data types in the input file\n",
    "hCols=hClick.dtypes\n",
    "print(\"Click history is made up of these data types:\\n\",hCols)\n",
    "#unique users in the input file by user id\n",
    "uniUserIDs=hClick.user_id.unique()\n",
    "print(\"\\nFirst 25 unique user_ids in click history are:\\n\",uniUserIDs[:25])\n",
    "#unique product ids in the input file\n",
    "uniProdIDs=hClick.product_id.unique()\n",
    "print(\"\\nFirst 25 unique product ids in click history are:\\n\",uniProdIDs[:25])\n",
    "#summary statistics of values in click history file\n",
    "dataset=hClick.describe()\n",
    "print(\"\\nSummary statistics of data values in click history dataset are:\\n\",dataset)\n",
    "#how many users actually clicked on the ads\n",
    "countClick=hClick.clicked.value_counts()    \n",
    "print(\"The amount of users that clicked on the ads were: \",countClick[1])     \n",
    "print(\"The amount of users that did not click on the ads were: \",countClick[0])     \n",
    "\n",
    "prodFeat=pd.read_csv(\"product_features.csv\")#reading in input file from file path listed above\n",
    "pFeatShape=prodFeat.shape #structure of input file (how many rows & columns)\n",
    "print(\"\\nProduct features has %5d rows & %1d columns\\n\" %(pFeatShape[0],pFeatShape[1]))\n",
    "pFeatSize=prodFeat.size #column * rows= numerical number\n",
    "print(\"This dataframe of the file data as a whole has %4d elements present\\n\" % pFeatSize)\n",
    "#data types in the input file\n",
    "pFeatCols=prodFeat.dtypes\n",
    "print(\"Product features is made up of these data types:\\n\",pFeatCols)\n",
    "#unique product ids in the input file\n",
    "pFeatUniID=prodFeat.product_id.unique()\n",
    "print(\"\\nFirst 25 unique product ids in product features are:\\n\",pFeatUniID[:25])\n",
    "#unique product categories in product features\n",
    "pFeatUniCat=prodFeat.category.unique()\n",
    "print(\"\\nThe unique product categories in product features are:\\n\",pFeatUniCat)\n",
    "#how many products were on sale or not\n",
    "pFeatSaleSat=prodFeat.on_sale.value_counts()\n",
    "print(\"\\nThe amount of products that were on sale were: \",pFeatSaleSat[1])\n",
    "print(\"The amount of products that were not on sale were: \",pFeatSaleSat[0])\n",
    "#summary statistics of values in product features file\n",
    "dataset2=prodFeat.describe()\n",
    "print(\"\\nSummary statistics of data values in product features dataset are:\\n\",dataset2)\n",
    "\n",
    "userFet=pd.read_csv(\"user_features.csv\")#reading in input file from file path listed above\n",
    "userShape=userFet.shape #structure of input file (how many rows & columns)\n",
    "print(\"\\nUser features has %5d rows & %1d columns\" %(userShape[0],userShape[1]))\n",
    "userSize=userFet.size #column * rows= numerical number\n",
    "print(\"This dataframe of the file data as a whole has %4d elements present\" % userSize)\n",
    "userCols=userFet.dtypes #data types in the input file\n",
    "print(\"\\nUser features is made up of these data types:\\n\",userCols)\n",
    "#unique users in user features file based on their user ids\n",
    "userUniIDs=userFet.user_id.unique()\n",
    "print(\"\\nFirst 25 unique user_ids in User features are:\\n\",userUniIDs[:25])\n",
    "#the rating score table of the number of clicks made before the ad was sent to the user by the user\n",
    "userUniClick=userFet.number_of_clicks_before.unique()\n",
    "print(\"\\nUser rating score of the number of clicks made previously by users of previous ads:\\n\",userUniClick)\n",
    "#count of previous ordered products by users\n",
    "userFetCountOr=userFet.ordered_before.value_counts()\n",
    "print(\"\\nHow many products were ordered before by users were: \",userFetCountOr[1])\n",
    "print(\"How many products were not ordered before by users were: \",userFetCountOr[0])\n",
    "#different types of interests listed by each user which might relate to future products purchased by them\n",
    "userFetUniInt=userFet.personal_interests.unique()\n",
    "print(\"\\nFirst 25 listed personal interests of each user:\\n\",userFetUniInt[:25])\n",
    "#determining how users listed no personal interests in the survey or have no personal interests that they can think of\n",
    "blankIntCount=0 #count index initalized\n",
    "for i in userFet.personal_interests:\n",
    "    if i==\"[]\": #checking if personal interests variable is blank\n",
    "        blankIntCount+=1\n",
    "print(\"\\nThe number of users that do not have personal interests listed are: \",blankIntCount)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46f422dc",
   "metadata": {},
   "source": [
    "Question 2: Data Cleaning and Preprocessing\n",
    "Clean and preprocess the datasets (such as missing values, outliers, dummy, merging etc.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3b17d16a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First few rows of new combined dataset of user features data and user click history data:\n",
      "     user_id number_of_clicks_before  ordered_before  \\\n",
      "0    104939                       2            True   \n",
      "1    104939                       2            True   \n",
      "2    104939                       2            True   \n",
      "3    104939                       2            True   \n",
      "4    104939                       2            True   \n",
      "5    101562                       2            True   \n",
      "6    101562                       2            True   \n",
      "7    101562                       2            True   \n",
      "8    101562                       2            True   \n",
      "9    101562                       2            True   \n",
      "10   102343                       2            True   \n",
      "11   102343                       2            True   \n",
      "12   102343                       2            True   \n",
      "13   102343                       2            True   \n",
      "14   102343                       2            True   \n",
      "\n",
      "                                   personal_interests  product_id  clicked  \n",
      "0   ['body', 'makeup', 'nail', 'hand', 'foot', 'me...        1212    False  \n",
      "1   ['body', 'makeup', 'nail', 'hand', 'foot', 'me...        1163    False  \n",
      "2   ['body', 'makeup', 'nail', 'hand', 'foot', 'me...        1687    False  \n",
      "3   ['body', 'makeup', 'nail', 'hand', 'foot', 'me...        1569    False  \n",
      "4   ['body', 'makeup', 'nail', 'hand', 'foot', 'me...        1195     True  \n",
      "5   ['men_skincare', 'men_fragrance', 'tools', 'sk...        1228    False  \n",
      "6   ['men_skincare', 'men_fragrance', 'tools', 'sk...        1178    False  \n",
      "7   ['men_skincare', 'men_fragrance', 'tools', 'sk...        1106    False  \n",
      "8   ['men_skincare', 'men_fragrance', 'tools', 'sk...        1098     True  \n",
      "9   ['men_skincare', 'men_fragrance', 'tools', 'sk...        1748    False  \n",
      "10                ['tools', 'makeup', 'foot', 'nail']        1270    False  \n",
      "11                ['tools', 'makeup', 'foot', 'nail']        1798    False  \n",
      "12                ['tools', 'makeup', 'foot', 'nail']        1677    False  \n",
      "13                ['tools', 'makeup', 'foot', 'nail']        1835    False  \n",
      "14                ['tools', 'makeup', 'foot', 'nail']        1420     True  \n",
      "\n",
      "First few rows of new combined dataset of user and product features data:\n",
      "     user_id number_of_clicks_before  ordered_before  \\\n",
      "0    104939                       2            True   \n",
      "1    101992                       1            True   \n",
      "2    110175                       0            True   \n",
      "3    111017                       1           False   \n",
      "4    105516                      6+           False   \n",
      "5    103186                       0           False   \n",
      "6    103133                       2            True   \n",
      "7    109085                      6+            True   \n",
      "8    108471                       4            True   \n",
      "9    107137                       4           False   \n",
      "10   107067                       4            True   \n",
      "11   109865                       2           False   \n",
      "12   106483                     NaN            True   \n",
      "13   104392                       4            True   \n",
      "14   103405                       0            True   \n",
      "15   110597                       1            True   \n",
      "16   109256                       1           False   \n",
      "17   110702                     NaN           False   \n",
      "18   100793                      6+           False   \n",
      "19   105584                       0           False   \n",
      "\n",
      "                                   personal_interests  product_id  clicked  \\\n",
      "0   ['body', 'makeup', 'nail', 'hand', 'foot', 'me...        1212    False   \n",
      "1                                                  []        1212    False   \n",
      "2      ['skincare', 'tools', 'foot', 'men_fragrance']        1212     True   \n",
      "3   ['fragrance', 'skincare', 'foot', 'hand', 'hai...        1212    False   \n",
      "4   ['hair', 'tools', 'nail', 'fragrance', 'foot',...        1212    False   \n",
      "5                                  ['makeup', 'hand']        1212    False   \n",
      "6                                                  []        1212    False   \n",
      "7                                            ['foot']        1212    False   \n",
      "8   ['makeup', 'body', 'skincare', 'men_fragrance'...        1212    False   \n",
      "9   ['hair', 'foot', 'fragrance', 'nail', 'makeup'...        1212    False   \n",
      "10  ['hair', 'nail', 'foot', 'tools', 'men_skincar...        1212    False   \n",
      "11            ['makeup', 'skincare', 'men_fragrance']        1212    False   \n",
      "12                                      ['fragrance']        1212    False   \n",
      "13     ['fragrance', 'foot', 'body', 'men_fragrance']        1212    False   \n",
      "14  ['men_skincare', 'tools', 'hair', 'nail', 'foo...        1212    False   \n",
      "15                                                 []        1212    False   \n",
      "16           ['men_fragrance', 'fragrance', 'makeup']        1212    False   \n",
      "17  ['men_fragrance', 'skincare', 'makeup', 'tools...        1212    False   \n",
      "18  ['makeup', 'men_fragrance', 'tools', 'nail', '...        1212    False   \n",
      "19  ['fragrance', 'hair', 'skincare', 'makeup', 'm...        1212    False   \n",
      "\n",
      "   category  on_sale  number_of_reviews  avg_review_score  \n",
      "0      hair     True                789          1.461363  \n",
      "1      hair     True                789          1.461363  \n",
      "2      hair     True                789          1.461363  \n",
      "3      hair     True                789          1.461363  \n",
      "4      hair     True                789          1.461363  \n",
      "5      hair     True                789          1.461363  \n",
      "6      hair     True                789          1.461363  \n",
      "7      hair     True                789          1.461363  \n",
      "8      hair     True                789          1.461363  \n",
      "9      hair     True                789          1.461363  \n",
      "10     hair     True                789          1.461363  \n",
      "11     hair     True                789          1.461363  \n",
      "12     hair     True                789          1.461363  \n",
      "13     hair     True                789          1.461363  \n",
      "14     hair     True                789          1.461363  \n",
      "15     hair     True                789          1.461363  \n",
      "16     hair     True                789          1.461363  \n",
      "17     hair     True                789          1.461363  \n",
      "18     hair     True                789          1.461363  \n",
      "19     hair     True                789          1.461363  \n",
      "\n",
      "The number of missing values in the combined dataset of user features,product features, and click history is:  0\n"
     ]
    }
   ],
   "source": [
    "#Question 2.)\n",
    "#both the user feature & click history datasets both have the same user_id columns in them\n",
    "combined_data=pd.merge(userFet,hClick,on='user_id')\n",
    "print(\"First few rows of new combined dataset of user features data and user click history data:\\n\",combined_data.head(15))\n",
    "#now the merged dataset created has product id to add product features dataset on\n",
    "combined_data2=pd.merge(combined_data,prodFeat,on='product_id')\n",
    "print(\"\\nFirst few rows of new combined dataset of user and product features data:\\n\",combined_data2.head(20))\n",
    "#handling na/missing values in the number_of_clicks before variable/column\n",
    "combined_data2['number_of_clicks_before'] =combined_data2['number_of_clicks_before'].fillna(0)\n",
    "#handling values less than 0 which should not happen because review scores should be greater of equal to 0\n",
    "combined_data2.loc[combined_data2['avg_review_score']<0,'avg_review_score']=0\n",
    "#changing number of clicks before values to only numeric values so 6+ has to get change to similar numeric value\n",
    "combined_data2.loc[combined_data2['number_of_clicks_before']=='6+','number_of_clicks_before']=6\n",
    "#all values of this variable are now all numeric so we can change the variable data type to integer type\n",
    "combined_data2=combined_data2.astype({'number_of_clicks_before': 'int_'})\n",
    "#all values of order_before need to numeric so we should change the data type of this variable to integer type\n",
    "combined_data2=combined_data2.astype({'ordered_before': 'int8'})\n",
    "#changing Order_before variable values to numeric ones to conduct proper analysis (0 & 1)\n",
    "combined_data2.loc[combined_data2['ordered_before']=='False','ordered_before']=0\n",
    "combined_data2.loc[combined_data2['ordered_before']=='True','ordered_before']=1\n",
    "#changing clicked variable values to numeric ones to conduct proper analysis 0 & 1\n",
    "combined_data2=combined_data2.astype({'clicked': 'int8'})\n",
    "combined_data2.loc[combined_data2['clicked']=='True','clicked']=1\n",
    "combined_data2.loc[combined_data2['clicked']=='False','clicked']=0\n",
    "#on sale variable values to numeric to conduct proper analysis 0 & 1\n",
    "combined_data2=combined_data2.astype({'on_sale': 'int8'})\n",
    "combined_data2.loc[combined_data2['on_sale']=='True','on_sale']=1\n",
    "combined_data2.loc[combined_data2['on_sale']=='False','on_sale']=0\n",
    "#reindexing variables to make slicing easier to be able to drop certain variables that I create dummy variables for\n",
    "combined_data2 = combined_data2.reindex(columns=\n",
    "                            ['user_id','product_id','personal_interests','category',\n",
    "                             'clicked','ordered_before','number_of_clicks_before','on_sale',\n",
    "                             'number_of_reviews','avg_review_score'])\n",
    "#getting dummy values for these categorical variables to be able to include them in our classification models\n",
    "catDumm=pd.get_dummies(combined_data2['category'],prefix='category')\n",
    "perDumm=pd.get_dummies(combined_data2['personal_interests'],prefix='personal_Interest',\n",
    "                       prefix_sep=':')\n",
    "#adding both categorical variables to our dataframe with numeric dummy values\n",
    "combined_data2=combined_data2.join([catDumm,perDumm])\n",
    "#dropping the original variables that we created dummy values for\n",
    "combined_data2=combined_data2.drop(['personal_interests','category'],axis=1)\n",
    "#making sure the newly created variables and existing variables that we changed values for too do not have missing values\n",
    "missingSum=combined_data2.isnull().values.sum()\n",
    "print(\"\\nThe number of missing values in the combined dataset of user features,product features, and click history is: \",\n",
    "      missingSum)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "690ca2dd",
   "metadata": {},
   "source": [
    "Question 3: Model Generation and Evaluation\n",
    "Please split the data into train and test sets with a ratio of 0.7:0.3. Build and optimize classification models you learned in this course."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "27265ed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Question 3.)\n",
    "#outcome variable for classification needs to be the variable clicked\n",
    "#because it is the variable that tells whether the user clicked on the ad,which we are trying to find out\n",
    "y=combined_data2.clicked\n",
    "#x/predictor variables should be variables that could be used to predict whether a user will click on the ad\n",
    "#these include variables such as ordered_before,number of clicks before,on sale, number of reviews, type of category product is\n",
    "X=combined_data2.iloc[:,3:5193]\n",
    "#splitting the data into train and test sets with a ratio of 0.7:0.3\n",
    "X_train,X_test,y_train,y_test=train_test_split(X,y,\n",
    "                                               test_size=0.3,random_state=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "88dbf6fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable Importance based on Decision tree model is:\n",
      "                                                variable  importance\n",
      "3                                     number_of_reviews    0.738610\n",
      "0                                        ordered_before    0.053167\n",
      "4                                      avg_review_score    0.043384\n",
      "5189                               personal_Interest:[]    0.021092\n",
      "2                                               on_sale    0.020310\n",
      "1                               number_of_clicks_before    0.015174\n",
      "5                                         category_body    0.005855\n",
      "6                                         category_foot    0.005340\n",
      "10                                      category_makeup    0.003972\n",
      "8                                         category_hair    0.003769\n",
      "1897                         personal_Interest:['hair']    0.003538\n",
      "2393                         personal_Interest:['hand']    0.003282\n",
      "12                                category_men_skincare    0.003061\n",
      "11                               category_men_fragrance    0.002371\n",
      "14                                    category_skincare    0.002339\n",
      "4285                         personal_Interest:['nail']    0.002310\n",
      "3794                 personal_Interest:['men_skincare']    0.002230\n",
      "13                                        category_nail    0.002187\n",
      "9                                         category_hand    0.001991\n",
      "2864                       personal_Interest:['makeup']    0.001477\n",
      "717                personal_Interest:['foot', 'makeup']    0.001179\n",
      "1412                    personal_Interest:['fragrance']    0.000948\n",
      "4977  personal_Interest:['tools', 'makeup', 'body', ...    0.000908\n",
      "374   personal_Interest:['body', 'nail', 'men_fragra...    0.000907\n",
      "1169          personal_Interest:['fragrance', 'makeup']    0.000906\n",
      "3005  personal_Interest:['men_fragrance', 'fragrance...    0.000828\n",
      "938                          personal_Interest:['foot']    0.000821\n",
      "2847  personal_Interest:['makeup', 'tools', 'hand', ...    0.000799\n",
      "786          personal_Interest:['foot', 'men_skincare']    0.000786\n",
      "2739  personal_Interest:['makeup', 'nail', 'fragranc...    0.000769\n",
      "Fitting 5 folds for each of 2 candidates, totalling 10 fits\n",
      "[CV] END .....................................n_neighbors=95; total time=  10.9s\n",
      "[CV] END .....................................n_neighbors=95; total time=  11.8s\n",
      "[CV] END .....................................n_neighbors=95; total time=  13.3s\n",
      "[CV] END .....................................n_neighbors=95; total time=  13.1s\n",
      "[CV] END .....................................n_neighbors=95; total time=  14.7s\n",
      "[CV] END ....................................n_neighbors=110; total time=  14.0s\n",
      "[CV] END ....................................n_neighbors=110; total time=  16.4s\n",
      "[CV] END ....................................n_neighbors=110; total time=  14.3s\n",
      "[CV] END ....................................n_neighbors=110; total time=  12.7s\n",
      "[CV] END ....................................n_neighbors=110; total time=  13.7s\n",
      "\n",
      "Variable importance based on Random Forest model is:\n",
      "                                            variable  importance\n",
      "3                                 number_of_reviews    0.186795\n",
      "4                                  avg_review_score    0.067631\n",
      "1                           number_of_clicks_before    0.027217\n",
      "0                                    ordered_before    0.016567\n",
      "2                                           on_sale    0.008673\n",
      "5189                           personal_Interest:[]    0.008135\n",
      "12                            category_men_skincare    0.003164\n",
      "3794             personal_Interest:['men_skincare']    0.003060\n",
      "6                                     category_foot    0.002985\n",
      "13                                    category_nail    0.002926\n",
      "2864                   personal_Interest:['makeup']    0.002777\n",
      "4285                     personal_Interest:['nail']    0.002630\n",
      "1412                personal_Interest:['fragrance']    0.002555\n",
      "938                      personal_Interest:['foot']    0.002543\n",
      "5188                    personal_Interest:['tools']    0.002514\n",
      "4760                 personal_Interest:['skincare']    0.002483\n",
      "1897                     personal_Interest:['hair']    0.002477\n",
      "10                                  category_makeup    0.002403\n",
      "2393                     personal_Interest:['hand']    0.002369\n",
      "3337            personal_Interest:['men_fragrance']    0.002311\n",
      "14                                category_skincare    0.002261\n",
      "474                      personal_Interest:['body']    0.002251\n",
      "15                                   category_tools    0.002216\n",
      "8                                     category_hair    0.002190\n",
      "5                                     category_body    0.001934\n",
      "9                                     category_hand    0.001904\n",
      "7                                category_fragrance    0.001719\n",
      "11                           category_men_fragrance    0.001370\n",
      "3149  personal_Interest:['men_fragrance', 'makeup']    0.001098\n",
      "576         personal_Interest:['foot', 'fragrance']    0.001041\n",
      "\n",
      "F1 Scores for the model are:\n",
      "Logistic regresion -  0.0\n",
      "Decision tree -  0.6441897654584222\n",
      "K-Nearest Neighbours -  0.613735721336906\n",
      "Random Forest -  0.5791814946619217\n",
      "\n",
      "Accuracy Scores for the model are:\n",
      "Logistic Regression -  0.6454570714087247\n",
      "Decision tree -  0.7527090858571825\n",
      "K-Nearest Neighbours -  0.7463184217838288\n",
      "Random Forest -  0.7371492081133648\n"
     ]
    }
   ],
   "source": [
    "#building classification models to predict whether a user will click on an advertisement sent by the company's website\n",
    "\n",
    "#1.) Logistic regression model \n",
    "lr=linear_model.LogisticRegression()\n",
    "lr.fit(X_train,y_train)\n",
    "lr_train_pred=lr.predict(X_train)\n",
    "lr_test_pred=lr.predict(X_test)\n",
    "\n",
    "log_acc=metrics.accuracy_score(y_test,lr_test_pred)\n",
    "log_f1=metrics.f1_score(y_test,lr_test_pred)\n",
    "\n",
    "#2.) Decision tree model\n",
    "DT=tree.DecisionTreeClassifier(max_depth=10,min_samples_split=5)\n",
    "DT.fit(X_train,y_train)\n",
    "DT_pred=DT.predict(X_test)\n",
    "\n",
    "dt_acc=metrics.accuracy_score(y_test,DT_pred)\n",
    "dt_f1=metrics.f1_score(y_test,DT_pred)\n",
    "\n",
    "#making a dataframe to show the feature importance of each predictor variable in the model \n",
    "varImport=pd.DataFrame({'variable':X.columns[:],\n",
    "              'importance':DT.feature_importances_})\n",
    "varImport=varImport.sort_values(by='importance',ascending=False)\n",
    "#showing top 30 variables that are the most important at predicting the outcome variable\n",
    "varImport=varImport.head(30)\n",
    "print(\"Variable Importance based on Decision tree model is:\\n\",varImport)\n",
    "\n",
    "#3.) K-Nearest Neighbours model\n",
    "#list of potential parameter values to get the most optimal K-Nearest neighbours model results for the model\n",
    "tunned_parameters={'n_neighbors':[95,110]}\n",
    "knn=KNeighborsClassifier(n_neighbors=5)\n",
    "knn_optimizer=GridSearchCV(knn,tunned_parameters,scoring='accuracy',\n",
    "                           return_train_score=False,verbose=2)\n",
    "knn_optimizer.fit(X_train,y_train)\n",
    "#for testing purpose to see which parameter value produced the best k-nearest neigbours model results score\n",
    "results=pd.DataFrame(knn_optimizer.cv_results_)\n",
    "knn_best_model=knn_optimizer.best_estimator_\n",
    "knn_best_pred=knn_best_model.predict(X_test)\n",
    "\n",
    "knn_acc=metrics.accuracy_score(y_test,knn_best_pred)\n",
    "knn_f1=metrics.f1_score(y_test,knn_best_pred)\n",
    "\n",
    "#4.) Random Forest model\n",
    "rfc=RandomForestClassifier(n_estimators=100,max_features=10,\n",
    "                           min_samples_split=5,random_state=2)\n",
    "rfc.fit(X_train,y_train)\n",
    "rfc_pred=rfc.predict(X_test)\n",
    "\n",
    "#making a dataframe to show the feature importance of each predictor variable in the model \n",
    "varImport2=pd.DataFrame({'variable':X.columns[:],\n",
    "              'importance':rfc.feature_importances_})\n",
    "varImport2=varImport2.sort_values(by='importance',ascending=False)\n",
    "#showing top 30 variables that are the most important at predicting the outcome variable\n",
    "varImport2=varImport2.head(30)\n",
    "print(\"\\nVariable importance based on Random Forest model is:\\n\",varImport2)\n",
    "\n",
    "rfc_acc=metrics.accuracy_score(y_test,rfc_pred)\n",
    "rfc_f1=metrics.f1_score(y_test,rfc_pred)\n",
    "\n",
    "#f1 & accuracy scores for each classification model\n",
    "print(\"\\nF1 Scores for the model are:\")\n",
    "print(\"Logistic regresion - \",log_f1)\n",
    "print(\"Decision tree - \",dt_f1)\n",
    "print(\"K-Nearest Neighbours - \",knn_f1)\n",
    "print(\"Random Forest - \",rfc_f1)\n",
    "\n",
    "print(\"\\nAccuracy Scores for the model are:\")\n",
    "print(\"Logistic Regression - \",log_acc)\n",
    "print(\"Decision tree - \",dt_acc)\n",
    "print(\"K-Nearest Neighbours - \",knn_acc)\n",
    "print(\"Random Forest - \",rfc_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ec10516",
   "metadata": {},
   "source": [
    "Question 4: Which model has the best performance? What have you learned from the models you built?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75a40971",
   "metadata": {},
   "source": [
    "   Through the project, the main objective we were trying to discover, was whether a user would click on an advertisement sent via email, based on certain factors. To best answer this objective, one had to address certain issues. These questions included whether we wanted a classification model that would handle false positives the same as false negatives. Or we wanted a classification model that would handle correctly predicted positive observations in relation to the amount of total predicted positive observations. Or did we want a classification model capable of handling correctly predicted positive observations in relation to all observations in the actual class. These questions all would involve either our model having a high accuracy, F1, and/or precision score.\n",
    "    In our case, our model would need to care about correctly predicting users click rate on advertisements, evenly. Which would mean the model could not lean on either side of predicting positive or negative values falsely. Thus, our classification model would need to have a relatively high accuracy or F1 score. Since, both scores consider false positive and false negative values, equally. Though models with high F1 scores would be able to handle uneven data distribution better than models that have high accuracy scores.\n",
    "    Once the type of score metric was determined, I had to choose which classification model would produce the highest accuracy and F1 scores. I ended up choosing from models that included the Logistic Regression, Naïve Bayes, Stochastic Gradient Descent, K-Nearest Neighbors, Decision Tree, Random Forest, and Support Vector Machine. From this selection, I knew I could eliminate three of the listed models from my analysis. Those three models included the Naïve Bayes, Stochastic Gradient Descent, and Support Vector Machine models.\n",
    "    The reason why I eliminated the Naïve Bayes model from my analysis was because the model is often times a bad estimator of data. Which often leads to the model loosely predicting the outcome variable, and ultimately having low F1 and accuracy scores. It is also a model that assigns zero probability values to categorical variables in test datasets that it could not find in training datasets. I did not need this issue since, there is no way of knowing whether certain values of a factor variable will show up in certain datasets. Because we are splitting the training and test datasets with a ratio of 0.70:0.30.\n",
    "    The Stochastic Gradient Descent model was removed from my analysis because the model was mostly efficient at fitting linear models. Which is great if we knew our data had solely linear correlations, but that was not the case in this instance. I did not know if the data would be best suited for a linear modelling approach. Another issue with this model was that it only updated after it went through every observation in the dataset. Thus, making it a slower learner model. Again, in our case, the dataframe being modeled on had numerous rows and columns. Which would cause this model to slowly converge data points.\n",
    "    The reason why I chose not to move further with the Support Vector Machine classification model was because of its computation inefficiency. I knew I would need to run a grid search on the kernel argument of the model. Which would take a long time to compile because of the size of our dataframe. Consequently, training speeds would be reduced tremendously. For this classification model to be useful, my dataset would need to have less data points, so my computer's memory would not be taxed storing all the support vectors. I also did not want to scale all my numeric variables, which is required by this model.\n",
    "    Once I eliminated the Naïve Bayes, Stochastic Gradient Descent, and Support Vector Machine models, I was left with the K-Nearest Neighbors, Decision Tree, Random Forest, and Logistic Regression classification methods. The Logistic Regression model made sense right way because of its fast approach at classifying unknown records, its easy to implement algorithm, and its concept of making no assumptions on distribution of factor variables in feature space. I knew I needed a classification model that did not require intensive computing power because as previously stated the dataset being trained was relatively large. Since the size of the dataset was increased by the dummy variables created from the original personal interests and product category variable.\n",
    "    The second model I knew I could add was the Decision Tree model. For one, this model could be used for both classification and regression problems. So, it was a very versatile method at handling both continuous and discrete values. All of which I had, in the dataset. For example, the review score variable in the dataset was continuous, while variables such as the number of clicks and the number of reviews a product had were discrete. The model could also handle nonlinear relationships well too. Which was another benefit because not all variables in the dataset had linear correlations. For one, I knew the dummy variables I created for product categories and user interests did not have linear relationships to whether the product was on sale or made a user click on a previous advertisement. Also, as with the Logistic Regression model was fast at computing its algorithm around data points. While being easy to use to visualize data. The importance model attribute showed its visualization strength by being able to display each variable’s level of importance at predicting the outcome variable. Lastly, normalization was not needed when I had to preprocess the original dataset.\n",
    "    The K-Nearest Neighbors model was the third classification model I decided to add to my analysis because of its ability to handle noisy training data. This capability was needed because some of my categorical variables had some difficult to understand values. Mainly because I had to change most of the categorical variables’ values to binary values. These variables included whether a product was ordered before, user interests, and product categories. Also, since the K-Nearest Neighbors model was a model with a memory-based approach, it always is evolving to newly collected data. The only downside for this model was that it is a lazy learner. So, as the dataset grows, its efficiency and speed declines drastically. Therefore, I decided to add this model behind the Logistic Regression and Decision tree model.\n",
    "    The final classification model I decided to use in my analysis was the Random Forest model. I decided to add this algorithm to my analysis, mainly because of its use of a meta-estimator to control over-fitting issues better than decision tree models. As a result, the Random Forest model tended to be more accurate than Decision Tree models. Also, the algorithms modeling approach that fits several decision trees on various sub-samples of a dataset, helps improve the model’s accuracy score, as well. However, it is a more complex model than a Decision Tree model. So, it’s sort of difficult to control the outcome of what the model does on a dataset. Often this is why the model is considered a black box approach for statistical modelling.\n",
    "    At the end of my analysis, using all five classification models listed above, I found out the Decision Tree model had the all-around best performance. The decision tree model had the highest F1 score of 64.30% and the highest accuracy score of 75.22%. While the K-Nearest Neighbors and Random Forest model came in 3rd and 4th, respectively. The Random Forest model came in last. Which was probably because I did not use the most optimal estimator value on the data. If I increased the number & size of decision trees to ensemble around the dataset, computation cost would have increased dramatically. However, if computation cost was not a factor, and I had more memory space available, I would be able to increase the depth and meta estimator by at least 40 percent. Thus, making the Random Forest model’s accuracy and F1 scores at least equal to the Decision Tree model’s score.  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
